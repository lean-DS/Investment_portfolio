# -*- coding: utf-8 -*-
"""Stockanalysis_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mr7eYs1wORNswHObAutUZr2M6e9i2vf0
"""

# stockanalysis_scraper.py
import re
import time
from typing import Dict, Iterable, List, Optional, Tuple
from urllib import robotparser
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

BASE = "https://stockanalysis.com"
SITEMAP_INDEX = urljoin(BASE, "/sitemap.xml")

# Polite session
_session = requests.Session()
_session.headers.update({
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/123.0 Safari/537.36"
})
DEFAULT_TIMEOUT = 12


def _allowed(path: str) -> bool:
    """Respect robots.txt."""
    rp = robotparser.RobotFileParser()
    rp.set_url(urljoin(BASE, "/robots.txt"))
    try:
        rp.read()
    except Exception:
        return False
    return rp.can_fetch(_session.headers.get("User-Agent", "*"), urljoin(BASE, path))


def _get(url: str) -> Optional[requests.Response]:
    try:
        r = _session.get(url, timeout=DEFAULT_TIMEOUT)
        if r.status_code == 200:
            return r
        return None
    except Exception:
        return None


def _iter_sitemaps() -> List[str]:
    """Return stock/ETF sitemaps from the index."""
    if not _allowed("/sitemap.xml"):
        return []
    r = _get(SITEMAP_INDEX)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "xml")
    urls = [loc.get_text(strip=True) for loc in soup.find_all("loc")]
    out = []
    for u in urls:
        lu = u.lower()
        if "stock" in lu or "etf" in lu:
            out.append(u)
    return out


def _extract_symbols_from_sitemap(sitemap_url: str) -> List[Tuple[str, str]]:
    """Parse a child sitemap → [(SYMBOL, kind='stocks'|'etf')]."""
    r = _get(sitemap_url)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "xml")
    out: List[Tuple[str, str]] = []
    for loc in soup.find_all("loc"):
        url = loc.get_text(strip=True)
        m = re.search(r"https?://stockanalysis\.com/(stocks|etf)/([A-Za-z0-9.\-]+)/?$", url)
        if m:
            kind, sym = m.group(1), m.group(2).upper()
            out.append((sym, kind))  # kind is 'stocks' or 'etf'
    return out


def fetch_all_symbols_from_sitemaps(types: Iterable[str] = ("stock", "etf"),
                                    max_per_type: int = 4000) -> Dict[str, List[str]]:
    """
    Scrape via sitemaps only (static, JS-free). Returns:
      {'stock': [...], 'etf': [...]}
    """
    maps = _iter_sitemaps()
    buckets: Dict[str, List[str]] = {"stock": [], "etf": []}
    if not maps:
        return buckets

    want_stock = "stock" in types
    want_etf = "etf" in types

    for sm in maps:
        low = sm.lower()
        is_stock_map = "stock" in low
        is_etf_map = "etf" in low
        if (is_stock_map and not want_stock) or (is_etf_map and not want_etf):
            continue

        time.sleep(0.6)  # polite delay between XML hits
        pairs = _extract_symbols_from_sitemap(sm)
        for sym, kind in pairs:
            if kind == "stocks" and want_stock and sym not in buckets["stock"]:
                buckets["stock"].append(sym)
            if kind == "etf" and want_etf and sym not in buckets["etf"]:
                buckets["etf"].append(sym)

    buckets["stock"] = buckets["stock"][:max_per_type]
    buckets["etf"] = buckets["etf"][:max_per_type]
    return buckets


# --------- Bond ETF detection (lightweight, title-based) ----------
BOND_KEYWORDS = re.compile(
    r"\b(bond|treasury|gilt|gilts|fixed income|aggregate|tips|inflation|credit|corporate|muni|municipal|duration|"
    r"short-term|short term|intermediate|long-term|long term)\b",
    flags=re.IGNORECASE
)


def _title_of(url_path: str) -> str:
    """Fetch <title> (small HTML parse, fast)."""
    if not _allowed(url_path):
        return ""
    r = _get(urljoin(BASE, url_path))
    if not r:
        return ""
    soup = BeautifulSoup(r.text, "lxml")
    return soup.title.get_text(" ", strip=True) if soup.title else ""


def fetch_bond_etfs_from_stockanalysis(max_check: int = 600, parallel: int = 6) -> List[str]:
    """
    Pull a limited set of ETF pages and classify by title → bond/treasury/gilt ETFs.
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    symbols = fetch_all_symbols_from_sitemaps(types=("etf",)).get("etf", [])
    if not symbols:
        return []

    candidates = symbols[:max_check]
    paths = [f"/etf/{sym.lower()}/" for sym in candidates]
    found: List[str] = []

    def worker(path: str) -> Optional[str]:
        time.sleep(0.2)  # throttle inside workers
        title = _title_of(path)
        if title and BOND_KEYWORDS.search(title):
            m = re.search(r"/etf/([a-z0-9.\-]+)/", path)
            if m:
                return m.group(1).upper()
        return None

    with ThreadPoolExecutor(max_workers=max(1, min(parallel, 8))) as ex:
        futs = [ex.submit(worker, p) for p in paths]
        for f in as_completed(futs):
            v = f.result()
            if v:
                found.append(v)

    # de-dupe, preserve order
    seen = set()
    out = []
    for s in found:
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out