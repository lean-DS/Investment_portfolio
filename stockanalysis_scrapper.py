# -*- coding: utf-8 -*-
"""Stockanalysis_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mr7eYs1wORNswHObAutUZr2M6e9i2vf0
"""

# stockanalysis_scraper.py
# stockanalysis_scraper.py
import re
import time
from typing import Dict, Iterable, List, Optional, Tuple
from urllib import robotparser
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

BASE = "https://stockanalysis.com"
SITEMAP_INDEX = urljoin(BASE, "/sitemap.xml")

_session = requests.Session()
_session.headers.update({
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/123.0 Safari/537.36"
})
DEFAULT_TIMEOUT = 12

def _allowed(path: str) -> bool:
    rp = robotparser.RobotFileParser()
    rp.set_url(urljoin(BASE, "/robots.txt"))
    try:
        rp.read()
    except Exception:
        return False
    return rp.can_fetch(_session.headers.get("User-Agent", "*"), urljoin(BASE, path))

def _get(url: str) -> Optional[requests.Response]:
    try:
        r = _session.get(url, timeout=DEFAULT_TIMEOUT)
        if r.status_code == 200:
            return r
        return None
    except Exception:
        return None

def _iter_sitemaps() -> List[str]:
    if not _allowed("/sitemap.xml"):
        return []
    r = _get(SITEMAP_INDEX)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "xml")
    urls = [loc.get_text(strip=True) for loc in soup.find_all("loc")]
    # Keep only stock/etf sitemaps
    return [u for u in urls if ("/stocks/" in u.lower() or "/etf/" in u.lower())]

def _extract_symbols_from_sitemap(sitemap_url: str) -> List[Tuple[str, str]]:
    r = _get(sitemap_url)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "xml")
    out: List[Tuple[str, str]] = []
    for loc in soup.find_all("loc"):
        url = loc.get_text(strip=True)
        m = re.search(r"https?://stockanalysis\.com/(stocks|etf)/([A-Za-z0-9.\-]+)/?$", url)
        if m:
            kind, sym = m.group(1), m.group(2).upper()
            out.append((sym, kind))  # kind is 'stocks' or 'etf'
    return out

def fetch_all_symbols_from_sitemaps(types: Iterable[str] = ("stock", "etf"),
                                    max_per_type: int = 5000) -> Dict[str, List[str]]:
    maps = _iter_sitemaps()
    buckets: Dict[str, List[str]] = {"stock": [], "etf": []}
    if not maps:
        return buckets

    want_stock = "stock" in types
    want_etf = "etf" in types

    # Walk through the child sitemaps, pick tickers
    for sm in maps:
        low = sm.lower()
        is_stock_map = "/stocks/" in low
        is_etf_map = "/etf/" in low
        if (is_stock_map and not want_stock) or (is_etf_map and not want_etf):
            continue

        time.sleep(0.5)  # polite throttle
        pairs = _extract_symbols_from_sitemap(sm)
        for sym, kind in pairs:
            if kind == "stocks" and want_stock:
                if sym not in buckets["stock"]:
                    buckets["stock"].append(sym)
            elif kind == "etf" and want_etf:
                if sym not in buckets["etf"]:
                    buckets["etf"].append(sym)

        if want_stock and len(buckets["stock"]) >= max_per_type and \
           want_etf   and len(buckets["etf"])   >= max_per_type:
            break

    buckets["stock"] = buckets["stock"][:max_per_type]
    buckets["etf"]   = buckets["etf"][:max_per_type]
    return buckets

# --------- Bond ETF detection (title-based, light HTML) ----------
BOND_KEYWORDS = re.compile(
    r"\b(bond|treasury|gilt|gilts|fixed income|aggregate|tips|inflation|credit|corporate|muni|municipal|duration|"
    r"short-term|short term|intermediate|long-term|long term|t-bill|t bill)\b",
    flags=re.IGNORECASE
)

def _title_of(path: str) -> str:
    if not _allowed(path):
        return ""
    r = _get(urljoin(BASE, path))
    if not r:
        return ""
    soup = BeautifulSoup(r.text, "lxml")
    return soup.title.get_text(" ", strip=True) if soup.title else ""

def fetch_bond_etfs_from_stockanalysis(max_check: int = 800, parallel: int = 6) -> List[str]:
    from concurrent.futures import ThreadPoolExecutor, as_completed
    symbols = fetch_all_symbols_from_sitemaps(types=("etf",), max_per_type=max_check*2).get("etf", [])
    if not symbols:
        return []

    candidates = symbols[:max_check]
    paths = [f"/etf/{sym.lower()}/" for sym in candidates]
    found: List[str] = []

    def worker(path: str) -> Optional[str]:
        time.sleep(0.15)
        title = _title_of(path)
        if title and BOND_KEYWORDS.search(title):
            m = re.search(r"/etf/([a-z0-9.\-]+)/", path)
            if m:
                return m.group(1).upper()
        return None

    with ThreadPoolExecutor(max_workers=max(1, min(parallel, 8))) as ex:
        futs = [ex.submit(worker, p) for p in paths]
        for f in as_completed(futs):
            v = f.result()
            if v:
                found.append(v)

    # de-dupe
    seen = set()
    out = []
    for s in found:
        if s not in seen:
            seen.add(s)
            out.append(s)
    return out
