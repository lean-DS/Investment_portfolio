# -*- coding: utf-8 -*-
"""Stockanalysis_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mr7eYs1wORNswHObAutUZr2M6e9i2vf0
"""

# stockanalysis_scraper.py
# stockanalysis_scraper.py
import re
import time
from typing import Dict, Iterable, List, Optional, Tuple
from urllib.parse import urljoin
from urllib import robotparser

import requests
from bs4 import BeautifulSoup

BASE = "https://stockanalysis.com"
SITEMAP_INDEX = urljoin(BASE, "/sitemap.xml")

_session = requests.Session()
_session.headers.update({
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/126.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
})
DEFAULT_TIMEOUT = 12

# --------------------------------------------------------------------
# Helpers
# --------------------------------------------------------------------
def _allowed(path: str) -> bool:
    rp = robotparser.RobotFileParser()
    rp.set_url(urljoin(BASE, "/robots.txt"))
    try:
        rp.read()
    except Exception:
        return False
    # StockAnalysis robots often return False for /sitemap.xml, but content is public.
    # We'll still check to be polite; if False, caller can decide what to do.
    return rp.can_fetch(_session.headers.get("User-Agent", "*"), urljoin(BASE, path))

def _get(url: str) -> Optional[requests.Response]:
    try:
        r = _session.get(url, timeout=DEFAULT_TIMEOUT)
        if r.status_code == 200:
            return r
    except Exception:
        pass
    return None

# --------------------------------------------------------------------
# Sitemaps → US stocks & ETFs (strict symbol filtering)
# --------------------------------------------------------------------
# Allow tickers like: A, AA, BRK.B, RDS-A, VOD, VOD.L (we drop .L later if using Stooq)
_TICKER_RE = re.compile(r"^[A-Za-z][A-Za-z0-9.\-]{0,9}$")

# Common non-ticker slugs that appear under /stocks/ or /etf/ in sitemaps
_DENY = {
    "SCREENER","COMPARE","INDUSTRY","EARNINGS-CALENDAR","MARKETS","NEWS",
    "PROVIDER","ECONOMY","INSIGHTS","LEARN","DIVIDENDS","IPO","LISTS","IDEAS",
    "ETFS","SECTORS","INDEX","BLOG","ABOUT","CONTACT","JOBS","BROKERS","TOOLS"
}

def _iter_sitemaps() -> List[str]:
    r = _get(SITEMAP_INDEX)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "xml")
    return [loc.get_text(strip=True) for loc in soup.find_all("loc")]

def _extract_symbols_from_sitemap(sitemap_url: str, max_take: Optional[int]=None) -> List[Tuple[str, str]]:
    """Return (symbol, kind) where kind in {'stocks','etf'} with strict filtering."""
    r = _get(sitemap_url)
    if not r:
        return []
    soup = BeautifulSoup(r.text, "xml")
    out: List[Tuple[str, str]] = []
    for loc in soup.find_all("loc"):
        url = loc.get_text(strip=True)
        m = re.search(r"https?://stockanalysis\.com/(stocks|etf)/([A-Za-z0-9.\-]+)/?$", url)
        if not m:
            continue
        kind, raw = m.group(1), m.group(2).upper()

        # Filter out known non-ticker slugs and force a ticker-like shape
        sym = raw.replace("/", "").upper()
        if sym in _DENY:
            continue
        if not _TICKER_RE.fullmatch(sym):
            continue

        out.append((sym, kind))
        if max_take and len(out) >= max_take:
            break
    return out

def fetch_all_symbols_from_sitemaps(types: Iterable[str] = ("stock","etf"),
                                    max_per_type: int = 5000,
                                    max_sitemaps: int = 10) -> Dict[str, List[str]]:
    sitemaps = _iter_sitemaps()
    stocks: List[str] = []
    etfs: List[str] = []
    for sm in sitemaps[:max_sitemaps]:
        time.sleep(0.3)
        pairs = _extract_symbols_from_sitemap(sm)
        for sym, kind in pairs:
            if kind == "stocks" and "stock" in types:
                if sym not in stocks:
                    stocks.append(sym)
            elif kind == "etf" and "etf" in types:
                if sym not in etfs:
                    etfs.append(sym)
        if len(stocks) >= max_per_type and len(etfs) >= max_per_type:
            break
    return {"stock": stocks[:max_per_type], "etf": etfs[:max_per_type]}

# --------------------------------------------------------------------
# UK universes (LSE + AIM list pages)
# --------------------------------------------------------------------
UK_LIST_PAGES = [
    "/list/london-stock-exchange/",
    "/list/london-stock-exchange-aim/",
]

def fetch_uk_epics_from_lists(max_pages: int = 2) -> List[str]:
    """Scrape EPICs from /quote/lon/<EPIC>/ and also scan table cells."""
    epics: set[str] = set()
    for idx, path in enumerate(UK_LIST_PAGES[:max_pages]):
        r = _get(urljoin(BASE, path))
        if not r:
            continue
        soup = BeautifulSoup(r.text, "lxml")

        # 1) Preferred: /quote/lon/XXXX/
        for a in soup.select("a[href]"):
            href = a["href"]
            m = re.search(r"/quote/(?:lon|lse)/([A-Za-z0-9.\-]+)/", href, flags=re.IGNORECASE)
            if m:
                epics.add(m.group(1).upper())

        # 2) Fallback: 1–5 letters (ignore all-digit codes)
        for td in soup.select("td"):
            t = td.get_text(strip=True).upper()
            if re.fullmatch(r"[A-Z]{1,5}(\.[A-Z])?", t):
                epics.add(t)

        time.sleep(0.2)
    return sorted(epics)
